# -*- coding: utf-8 -*-
"""EE610_Assignment_2_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1txSxwvmKLzDYn0xspUxLiQZWGt1XgCYS

# Kalpit Borkar, 200070029
"""

# Importing libraries
import numpy as np
import cv2
import matplotlib.pyplot as plt
from google.colab.patches import cv2_imshow
from PIL import Image, ImageFilter
from scipy import ndimage, misc
from sklearn.model_selection import train_test_split
import glob
from sklearn.svm import SVR

# Mount drive
from google.colab import drive
drive.mount('/content/drive')

"""# Problem 1
1. Take a night time photo with your phone camera (if your phone camera is already very good, then borrow
someone else’s phone) and try to enhance it by manipulating its histogram and reducing noise. It should still
look natural and like a night time photo, but more details should be apparent than before. [2]
"""

img1 = cv2.imread('night_image1.jpg') # Read the image
cv2_imshow(img1) # Display the image

# Get the grayscale image
img1_gray = cv2.imread('night_image1.jpg', 0) # Read the image
cv2_imshow(img1_gray) # Display the image

# Calculate the histogram
hist1 = cv2.calcHist([img1_gray],[0],None,[256],[0,256])

# Display the histogram
plt.figure()
plt.title('Greyscale histogram')
plt.xlabel('Bins')
plt.ylabel('Number of pixels')
plt.plot(hist1)
plt.xlim([0, 256])

# Try histogram equalization
equal_img1 = cv2.equalizeHist(img1_gray)
cv2_imshow(equal_img1)

# Calculate equalized histogram
equal_hist1 = cv2.calcHist([equal_img1],[0],None,[256],[0,256])

# Display the histogram
plt.figure()
plt.title('Greyscale histogram')
plt.xlabel('Bins')
plt.ylabel('Number of pixels')
plt.plot(equal_hist1)
plt.xlim([0, 256])

# Try to remove the noise
removed_noise = equal_img1.copy() # Create copy

for i in range(2250):
  for j in range(4000):
    if(removed_noise[i][j] > 230): # If intensity if above threshold, make it equal to 255
      removed_noise[i][j] = 255

# Check noise free histogram
noise_free_hist = cv2.calcHist([removed_noise],[0],None,[256],[0,256])

# Display noise free histogram
plt.figure()
plt.title('Greyscale histogram')
plt.xlabel('Bins')
plt.ylabel('Number of pixels')
plt.plot(noise_free_hist)
plt.xlim([0, 256])

# Check noise free image
cv2_imshow(removed_noise)

# Try gamma correction

# Define function for gamma correction
# https://pyimagesearch.com/2015/10/05/opencv-gamma-correction/
def gammaCorrection(src, gamma):
    ig = 1 / gamma
    table = [((i / 255) ** ig) * 255 for i in range(256)]
    table = np.array(table, np.uint8)
    return cv2.LUT(src, table)

# Try gamma correction
gammaCorrected1 = gammaCorrection(equal_img1, 1.2)
cv2_imshow(gammaCorrected1)

# This did'nt work well

# Try blurring - the image might look smoother
blurred_img1 = cv2.GaussianBlur(removed_noise,(9,9),0)
cv2_imshow(blurred_img1)

# Try bilateral filter - the image looks continuous, but not sharp
bilateral = cv2.bilateralFilter(removed_noise, 30, 30, 30)

cv2_imshow(bilateral)

# Try sharpening the image
kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]]) # Sharpening kernel
im = cv2.filter2D(bilateral, -1, kernel)
cv2_imshow(im)

"""# Problem 2
2. Remove the newspaper-ink-dot effect and try to make the image at https://momofilmfest.com/wpcontent/
uploads/2020/01/newspaper-dots.jpg more natural-looking. [2]
"""

newspaper_img = cv2.imread('newspaper-dots.jpg') # Read the image
cv2_imshow(newspaper_img) # Display the image

# Plot the magnitude spectrum of the image 
# https://docs.opencv.org/3.4/de/dbc/tutorial_py_fourier_transform.html
newspaper_grey = cv2.imread('newspaper-dots.jpg',0)
f = np.fft.fft2(newspaper_grey) # Take fft
fshift = np.fft.fftshift(f)
magnitude_spectrum = 20*np.log(np.abs(fshift)) # Plot magnitude spectrum
plt.subplot(121),plt.imshow(newspaper_img, cmap = 'gray')
plt.title('Input Image'), plt.xticks([]), plt.yticks([])
plt.subplot(122),plt.imshow(magnitude_spectrum, cmap = 'gray')
plt.title('Magnitude Spectrum'), plt.xticks([]), plt.yticks([])
plt.show()

# Find the bright spots in the magnitude spectrum and mask them
delta = 25 # Size of circular mask
#  The point 40, 60 was found manually using cropping
for i in range(0,477, 60): 
  for j in range(0,320, 40):
    if((abs(i-238)<30 and abs(j-160)<30) or ((i/60)%2!=(j/40)%2)): # Masking locations
      continue
    for k in range(-delta+i, i+delta+1):
      for h in range(j-delta, j+delta+1):
        if((k-i)**2+(h-j)**2<delta**2): # Circle masking
          fshift[k, h] = fshift[k, h]/abs(fshift[k, h]);

# Check magnitude after masking
magnitude_spectrum = 20*np.log(np.abs(fshift))
plt.imshow(magnitude_spectrum, cmap = 'gray')

# Take inverse fourier transform
# https://docs.opencv.org/3.4/de/dbc/tutorial_py_fourier_transform.html
f_ishift = np.fft.ifftshift(fshift)
img_back = np.fft.ifft2(f_ishift)
img_back = np.real(img_back)


cv2_imshow(img_back)

# Check image shape
img_back.shape

# Try median filter to remove salt and pepper noise
# OpenCV Median filter is crashing on colab, so I made my own below

median_filter = img_back
for i in range(2, 475):
  for j in range(2, 318):
    arr = []
    for k in range(-1, 2):
      for l in range(-1, 2):
        arr.append(img_back[i - k][j - l])
    arr.sort()
    median_filter[i][j] = arr[4]

# But the image seems to be blurred
cv2_imshow(median_filter)

# Try scipy median filter (opencv median filter crashes colab)
median_filter2 = ndimage.median_filter(img_back, size=2)
cv2_imshow(median_filter2)
# The resultant image is quite blurred as well

# Try average filter
avg_blur = cv2.blur(img_back, (3,3))
cv2_imshow(avg_blur)

# Try downsampling
downsample = cv2.pyrDown(img_back)
cv2_imshow(downsample)

"""# Problem 3:
3. Triton is the largest of Neptune’s satellites (moons), and is the most unusual in our solar system as it orbits
its planet in the opposite direction to the planet’s rotation. One of its images taken by Voyager 2 in 1989 is
hosted at https://www.wired.com/images_blogs/thisdayintech/2009/08/triton_voyager2.jpg. This picture
seems to have some subtle horizontal (and possibly vertical) scan lines. Enhance this image by removing the
scanning artifacts. Also try to reveal more details by manipulating the histogram. You can choose to work on
a sub-image or a gray scale version, but for full marks maintain the color balance (ratio of R:G:B at each pixel)
and work with all three channels and show the final result on the entire (or a large portion of the) image. [3]
"""

# Read the image
img3 = cv2.imread('triton_voyager2.jpg')
img3_grey = cv2.imread('triton_voyager2.jpg', 0) # Grayscale image
cv2_imshow(img3)

# Compute Fourier Transform
# https://docs.opencv.org/3.4/de/dbc/tutorial_py_fourier_transform.html
f = np.fft.fft2(img3_grey)
fshift = np.fft.fftshift(f)
magnitude_spectrum3 = 20*np.log(np.abs(fshift)) # Check the magnitude spectrum

plt.subplot(121),plt.imshow(img3_grey, cmap = 'gray')
plt.title('Input Image'), plt.xticks([]), plt.yticks([])
plt.subplot(122),plt.imshow(magnitude_spectrum3, cmap = 'gray')
plt.title('Magnitude Spectrum'), plt.xticks([]), plt.yticks([])
plt.show()

# Split images int bgr channels to maintain RGB ratio
b,g,r = cv2.split(img3)

# Blue channel
# Check the magnitude spectrum
f = np.fft.fft2(b)
fshift = np.fft.fftshift(f)
magnitude_spectrum3 = 20*np.log(np.abs(fshift))

plt.subplot(121),plt.imshow(b)
plt.title('Input Image'), plt.xticks([]), plt.yticks([])
plt.subplot(122),plt.imshow(magnitude_spectrum3)
plt.title('Magnitude Spectrum'), plt.xticks([]), plt.yticks([])
plt.show()


# Add masking to remove horizontal lines in fourier transform
remove_size = 7 # Masking size
for i in range(330):
  for j in range(-remove_size, remove_size+1):
    fshift[280-j][i] = 0

for i in range(350, 680):
  for j in range(-remove_size, remove_size+1):
    fshift[280-j][i] = 0

# Take inverse fourier transform
f_ishift = np.fft.ifftshift(fshift)
img_back_b = np.fft.ifft2(f_ishift)
img_back_b = np.real(img_back_b)

# Check masked magnitude spectrum
magnitude_spectrum_b = 20*np.log(np.abs(fshift))
cv2_imshow(magnitude_spectrum_b)

# Same as above, but for green channel
# The comments would be similar

# Green channel
# Check the magnitude spectrum
f = np.fft.fft2(g)
fshift = np.fft.fftshift(f)
magnitude_spectrum3 = 20*np.log(np.abs(fshift))

plt.subplot(121),plt.imshow(g)
plt.title('Input Image'), plt.xticks([]), plt.yticks([])
plt.subplot(122),plt.imshow(magnitude_spectrum3)
plt.title('Magnitude Spectrum'), plt.xticks([]), plt.yticks([])
plt.show()

remove_size = 7
for i in range(330):
  for j in range(-remove_size, remove_size+1):
    fshift[280-j][i] = 0

for i in range(350, 680):
  for j in range(-remove_size, remove_size+1):
    fshift[280-j][i] = 0

f_ishift = np.fft.ifftshift(fshift)
img_back_g = np.fft.ifft2(f_ishift)
img_back_g = np.real(img_back_g)


magnitude_spectrum_g = 20*np.log(np.abs(fshift))
cv2_imshow(magnitude_spectrum_g)

# Same as above, but for red channel
# The comments would be similar

# Red channel
# Check the magnitude spectrum
f = np.fft.fft2(r)
fshift = np.fft.fftshift(f)
magnitude_spectrum3 = 20*np.log(np.abs(fshift))

plt.subplot(121),plt.imshow(r)
plt.title('Input Image'), plt.xticks([]), plt.yticks([])
plt.subplot(122),plt.imshow(magnitude_spectrum3)
plt.title('Magnitude Spectrum'), plt.xticks([]), plt.yticks([])
plt.show()

remove_size = 7
for i in range(330):
  for j in range(-remove_size, remove_size+1):
    fshift[280-j][i] = 0

for i in range(350, 680):
  for j in range(-remove_size, remove_size+1):
    fshift[280-j][i] = 0

f_ishift = np.fft.ifftshift(fshift)
img_back_r = np.fft.ifft2(f_ishift)
img_back_r = np.real(img_back_r)


magnitude_spectrum_r = 20*np.log(np.abs(fshift))
cv2_imshow(magnitude_spectrum_r)

# Merge BGR channels and check the merged image
merged = cv2.merge([img_back_b, img_back_g, img_back_r])
cv2_imshow(merged)

# Sharpen image to enhance details
kernel = np.array([[0,-1,0], [-1,5,-1], [0,-1,0]])
final5 = cv2.filter2D(merged, -1, kernel)
cv2_imshow(final5)

"""# Problem 4
4. A picture of a car blurred due to the relative motion of the camera is given at https://www.ee.iitb.ac.in/~asethi/Dump/MakeNumberPlateReadable.jpg. Restore and enhance the picture to reveal the numbers and letters on the number (license) plate. [3]
"""

img4 = cv2.imread('MakeNumberPlateReadable.jpg')
img4_gray = cv2.imread('MakeNumberPlateReadable.jpg', 0)
cv2_imshow(img4)

# https://www.researchgate.net/figure/Wiener-Filter-implementation-using-Python_fig3_332574579
def Wiener_Filter(img, kernel, K):
  s = np.sum(kernel)
  kernel = kernel/s
  dummy = np.copy(img)
  dummy = np.fft.fft2(dummy)
  kernel = np.fft.fft2(kernel, s = img.shape)
  kernel = np.conj(kernel)/ (np.abs(kernel)**2 + K)
  dummy = dummy*kernel
  dummy = np.abs(np.fft.ifft2(dummy))
  return dummy

# Try with gray scale image
from skimage import restoration
psf = np.zeros((49, 49))
psf[22:24, 21:] = 1/42
gray = restoration.wiener(img4_gray[600:800, 900:1200].copy(), psf, 1, clip = False)
cv2_imshow(gray)

# Try with color - idea was suggested by Sameep Chattopadhyay
from skimage import restoration
psf = np.zeros((49, 49))
psf[22:24, 21:] = 1/42

b = restoration.wiener(img4[600:800, 900:1200, 0].copy(), psf, 1, clip = False)
g = restoration.wiener(img4[600:800, 900:1200, 1].copy(), psf, 1, clip = False)
r = restoration.wiener(img4[600:800, 900:1200, 2].copy(), psf, 1, clip = False)
rgb = np.dstack((b,g,r))

cv2_imshow(rgb)

# Sharpen image to enhance details
kernel1 = np.array([[0,-1,0], [-1,5,-1], [0,-1,0]])
final1 = cv2.filter2D(rgb, -1, kernel)
cv2_imshow(final1)

"""# Problem 5
5. ML-based image restoration:
a. Select a few images from your personal collection, and divide them into good and bad images [1]
b. For bad images, try to figure out the degradation process (contrast, brightness, blurring, noise) [2]
c. Split the good images into training and validation sets (by image) [0]
d. Degrade the good images using the degradation process - contrast and brightness change, blurring,
noise. [2]
e. Create a table of size NxW2 or Nx3W2 where N is the number of patches of size WxWx1 or WxWx3
mined from the degraded versions of the training images. [2] Correspondingly, mine their associated
original central pixel of size Nx3 or Nx1. Tips:
i. You may only want to mine a few patches from random locations from each image, instead
of mining all possible patches
ii. You may want to predict the residual of the difference between the original and degraded
central pixel, and add it later to the degraded image
iii. You may want to first change the contrast and brightness of the original image, because it
cannot be predicted very well from this machine learning process
f. Similarly, create tables of size MxW2 or Mx3W2 and Mx3 or Mx1 for the validation images [0]
g. Train a regression model, e.g. support vector machine regression with RBF kernel and vary the
hyperparameters to check the performance on the validation dataset. You can modify and use the
following commands: [3]
i. from sklearn.svm import SVR
ii. regressor = SVR(kernel='rbf', C=1, epsilon=0.1, gamma=’scale’) #change hyper parameters C,
epsilon, and gamma as powers of 10 from 1e-2 to 1e2 to see which combo works best
iii. regressor.fit(X,y) # Here, X is your NxW2 or Nx3W2 array, and y is Nx1 or Nx3 array
iv. y_pred = regressor.predict(X_val) #X_val is MxW2 or Mx3W2 array
v. # Now compute MSE between y_pred and y_val, where y_val is Mx1 or Mx3 array
h. Now apply this model (regressor.predict) to a few of your bad images (every overlapping patch of
size WxW) to see if these can be restored. [2]
i. Check which value of W gives good results. [2]
"""

# Load the images
imgs5 = [] # List containing the data
# gdrive_path = '/content/drive/My Drive/good/'


path = "/content/drive/My Drive/good/*.*"
for file in glob.glob(path):
  img = cv2.imread(file)
  imgs5.append(img) # Add images to the list

cv2_imshow(imgs5[0]) # Check one of the images

# Define image degradations

def GaussianBlur(img, kernel_size):
  return cv2.GaussianBlur(img, (k,k), 0)

# https://stackoverflow.com/questions/39308030/how-do-i-increase-the-contrast-of-an-image-in-python-opencv
def ChangeContrast(img):
  # converting to LAB color space
  lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)
  l_channel, a, b = cv2.split(lab)

  # Applying CLAHE to L-channel
  # feel free to try different values for the limit and grid size:
  clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
  cl = clahe.apply(l_channel)

  # merge the CLAHE enhanced L-channel with the a and b channel
  limg = cv2.merge((cl,a,b))

  # Converting image from LAB Color model to BGR color spcae
  enhanced_img = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)

  # Stacking the original image with the enhanced image
  result = np.hstack((img, enhanced_img))
  return result

# https://stackoverflow.com/questions/22937589/how-to-add-noise-gaussian-salt-and-pepper-etc-to-image-in-python-with-opencv
def AddSaltPepperNoise(img):
  row,col,ch = img.shape
  s_vs_p = 0.5
  amount = 0.004
  out = np.copy(img)
  # Salt mode
  num_salt = np.ceil(amount * img.size * s_vs_p)
  coords = [np.random.randint(0, i - 1, int(num_salt))
          for i in img.shape]
  out[coords] = 1

  # Pepper mode
  num_pepper = np.ceil(amount* img.size * (1. - s_vs_p))
  coords = [np.random.randint(0, i - 1, int(num_pepper))
          for i in img.shape]
  out[coords] = 0
  return out

# Split data in train and validation sets
# First 35 images are train, rest are validation

# List for degraded images
degraded_imgs = []

# Apply different combinations of degradations to the images
for i in range(len(imgs5)):
  if(i >= 0 and i <= 7):
    img = GaussianBlur(imgs5[i], 3)
    degraded_imgs.append(img)

  elif(i >= 8 and i <= 13):
    img = GaussianBlur(imgs5[i],3)
    img = AddSaltPepperNoise(imgs5[i])
    degraded_imgs.append(img)

  elif(i >= 14 and i <= 20):
    img = GaussianBlur(imgs5[i],3)
    img = ChangeContrast(imgs5[i])
    degraded_imgs.append(img)

  elif(i >= 21 and i <= 30):
    img = AddSaltPepperNoise(imgs5[i])
    degraded_imgs.append(img)
                                  
  elif(i >= 31 and i <= 40):
    img = ChangeContrast(imgs5[i])
    degraded_imgs.append(img)
  
  else:
    img = ChangeContrast(imgs5[i])
    img = AddSaltPepperNoise(imgs5[i])
    degraded_imgs.append(img)

# Covert to grayscale
degraded_gray = []
for i in range(len(degraded_imgs)):
  gray = cv2.cvtColor(degraded_imgs[i], cv2.COLOR_BGR2GRAY)
  degraded_gray.append(gray)

# Covert to grayscale
imgs5_gray = []
for i in range(len(imgs5)):
  gray = cv2.cvtColor(imgs5[i], cv2.COLOR_BGR2GRAY)
  imgs5_gray.append(gray)

# print(len(degraded_gray),len(imgs5_gray))

W = 2 # Define patch size
wxw = [] # Flattened patches of dimensions wxw
center = [] # Center


for i in range(47):
  for z in range(500):  
    for j in range(W):
        center_pixel_x = np.random.choice(range(W//2, imgs5_gray[i].shape[0]-W//2))
        center_pixel_y = np.random.choice(range(W//2, imgs5_gray[i].shape[1]-W//2))

        arr = []

        for k in range(-W//2, W//2+1):
            for l in range(-W//2, W//2+1):
                arr.append(degraded_gray[i][center_pixel_x + k][center_pixel_y + l])

        # arr = image[center_pixel_x-1:center_pixel_x+2, center_pixel_y-1: center_pixel_y+2]

        wxw.append(arr)
        # print(center_pixel_x,center_pixel_y)
        center.append(imgs5_gray[i][center_pixel_x][center_pixel_y])

wxw = np.array(wxw)
print(wxw.shape)
center = np.array(center)
X = wxw
Y = center

X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.33,random_state=69)

regressor = SVR(kernel='rbf', C=50, epsilon=0.1, gamma='scale')

# print(regressor.score(X,y))
regressor.fit(X_train,Y_train)
print(regressor.score(X_test,Y_test))

# Credits to Dadhichi
patches = []
degraded_test_gray = degraded_gray[1]
for i in range(200,400):
    for j in range(120,500):
        arr = []
        for k in range(-W//2,W//2+1):
            for l in range(-W//2,W//2+1):
                arr.append(degraded_test_gray[i+k][j+l])
        patches.append(arr)


x = np.array(patches)
print(x.shape)
y_pred = regressor.predict(x)
predicted_img = y_pred.reshape((200,380))

plt.figure(figsize=(100,100))
plt.subplot(131), plt.imshow(imgs5_gray[1][200:400,120:500],cmap='gray')
plt.subplot(132), plt.imshow(degraded_test_gray[200:400,120:500],cmap='gray')
plt.subplot(133), plt.imshow(predicted_img,cmap='gray')
plt.show()

test_img = imgs5[1]
cv2_imshow(test_img)

degraded_test_img = degraded_imgs[1]
cv2_imshow(degraded_test_img)

"""# References and Credits

- Problem 5 was discussed with my friend Dadhichi 20D070083
- Problem 4 was discussed with my friend Sameep 20D070067

- Links:
  - https://pyimagesearch.com/2015/10/05/opencv-gamma-correction/
  - https://docs.opencv.org/3.4/de/dbc/tutorial_py_fourier_transform.html
  - https://docs.opencv.org/3.4/de/dbc/tutorial_py_fourier_transform.html
  - https://docs.opencv.org/3.4/de/dbc/tutorial_py_fourier_transform.html
  - https://www.researchgate.net/figure/Wiener-Filter-implementation-using-Python_fig3_332574579
  - https://stackoverflow.com/questions/39308030/how-do-i-increase-the-contrast-of-an-image-in-python-opencv
  - https://stackoverflow.com/questions/22937589/how-to-add-noise-gaussian-salt-and-pepper-etc-to-image-in-python-with-opencv

"""

